<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/kavysabu1996/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/kavysabu1996/" rel="alternate" type="text/html" /><updated>2023-05-24T17:59:41+05:30</updated><id>http://localhost:4000/kavysabu1996/feed.xml</id><title type="html">ksGitVerse</title><subtitle>Work in progress!!!!!!!!!</subtitle><entry><title type="html">AutoEncoder - Reconstructuction of MNIST</title><link href="http://localhost:4000/kavysabu1996/jekyll/update/2023/04/27/AE-Reconstruction-MNIST.html" rel="alternate" type="text/html" title="AutoEncoder - Reconstructuction of MNIST" /><published>2023-04-27T12:27:33+05:30</published><updated>2023-04-27T12:27:33+05:30</updated><id>http://localhost:4000/kavysabu1996/jekyll/update/2023/04/27/AE-Reconstruction-MNIST</id><content type="html" xml:base="http://localhost:4000/kavysabu1996/jekyll/update/2023/04/27/AE-Reconstruction-MNIST.html"><![CDATA[]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/kavysabu1996/assets/images/coldplay.jpg" /><media:content medium="image" url="http://localhost:4000/kavysabu1996/assets/images/coldplay.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">DETR - DEtection Transformer</title><link href="http://localhost:4000/kavysabu1996/jekyll/update/2023/03/14/DETR.html" rel="alternate" type="text/html" title="DETR - DEtection Transformer" /><published>2023-03-14T12:27:33+05:30</published><updated>2023-03-14T12:27:33+05:30</updated><id>http://localhost:4000/kavysabu1996/jekyll/update/2023/03/14/DETR</id><content type="html" xml:base="http://localhost:4000/kavysabu1996/jekyll/update/2023/03/14/DETR.html"><![CDATA[<p><code class="language-plaintext highlighter-rouge">DETR</code> aka DEtection TRansformer is a state-of-art algorithm proposed by Meta(formerly known as FaceBook) in 2020. Through DETR they introduced new method which considers object detection as a direct set prediction problem. This was the first time that Object Detection field incorporated the tansformer mechanism and this integration brought huge impact.</p>

<h3 id="what-is-object-detection">What is Object Detection</h3>

<p>We humans have ability to watch and identify things. For example, consider this image</p>

<div style="text-align:center">
    <img src="/kavysabu1996/assets/images/DETR/fig1.png" alt="Image" style="max-width:400px;" />
</div>

<div style="text-align:center">
    fig1.
    <a href="https://stock.adobe.com/in/search?k=dog+cat+and+bird&amp;asset_id=81951964">image source</a>
</div>

<p>You can identify each object by simply looking at it. Left most is dog, cat at the center, and a bird. But what about machines?<br /> 
Humans’ve developed techniques for machines to identify and locate things. This process known as Object Detection. In object detection machine detects the presence of object, localize it through drawing bounding boxes and identify the class of the object. An exmaple of object detection is given below.</p>

<div style="text-align:center">
    <img src="/kavysabu1996/assets/images/DETR/fig1_box.png" alt="Image" style="max-width:400px;" />
</div>

<div style="text-align:center">
    fig2.output of DETR
</div>

<p>Don’t confuse it with image classification. In classification problems, the task of machine is to identify class of the given object, there is no need of localization.</p>

<p>Here they framed object detection as a direct-set prediciton problem. Which means the model directly predicts a set of items rather than predicting individual items separately. Ordinary object detection methods perfomed their tak by predicting bounding boxes and class labels for individual objects in an image, whereas DETR predicts the entire set of objects in a single forward pass. This method eleminated requirement of prior knowledge about the number of objects present in the image. This eliminates the need for techniques like anchor boxes, non-maximum suppression, or region proposal networks that are commonly used in traditional object detection methods.</p>

<h3 id="architecture">Architecture</h3>

<div style="text-align:center">
    <img src="/kavysabu1996/assets/images/DETR/detr_diagram.png" alt="DETR diagram" style="max-width:800px;" />
</div>

<div style="text-align:center">
    fig3.
    <a href="https://arxiv.org/pdf/2005.12872.pdf">image source</a>
</div>

<p>DETR architecture contains main three components</p>
<ul>
  <li>CNN backbone - to extract feature representation</li>
  <li>Transformer</li>
  <li>Feed-forward Network - to make final detectio prediction.</li>
</ul>

<p><strong>Backbone</strong><br />
In the first stage, image features are extracted using backbone layer. We can use any CNN models for this task. Most commonly used are RestNet-50 or ResNet-101.
These feature vectors are then flattened, added with positional encoding vector and fed into the Transformer.</p>

<p><strong>Transformer</strong><br />
Transformer consits to encoder and decoder blocks. Encoder encode the spatial information of the input image into a set of fixed-length feature representations. These features capture the visual content and contextual information needed for object detection. The decoder generate a set of detection outputs based on the encoded features. It attends to the relevant features, performs object classification, predicts the number of objects, and regresses the bounding box coordinates.</p>

<p><strong>Prediction feed-forward networks (FFNs)</strong><br /> 
The final prediction is computed by perceptron and a linear projection layer. The FFN predicts the normalized center coordinates, height and width of the box w.r.t. the input image, and the linear layer predicts the class label using a softmax function.</p>

<p>This diagram explains the whole process</p>
<div style="text-align:center">
    <img src="/kavysabu1996/assets/images/DETR/detr_diagram2.png" alt="DETR diagram" style="max-width:800px;" />
</div>

<div style="text-align:center">
    fig4.
    <a href="https://arxiv.org/pdf/2005.12872.pdf">image source</a>
</div>

<h3 id="about-implementation">About implementation</h3>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[DETR aka DEtection TRansformer is a state-of-art algorithm proposed by Meta(formerly known as FaceBook) in 2020. Through DETR they introduced new method which considers object detection as a direct set prediction problem. This was the first time that Object Detection field incorporated the tansformer mechanism and this integration brought huge impact.]]></summary></entry><entry><title type="html">Understanding Stable Diffusion: A Comprehensive Guide to the Fundamentals of the Powerful Deep Learning Technique</title><link href="http://localhost:4000/kavysabu1996/jekyll/update/2023/02/27/stable-diffusion.html" rel="alternate" type="text/html" title="Understanding Stable Diffusion: A Comprehensive Guide to the Fundamentals of the Powerful Deep Learning Technique" /><published>2023-02-27T12:27:33+05:30</published><updated>2023-02-27T12:27:33+05:30</updated><id>http://localhost:4000/kavysabu1996/jekyll/update/2023/02/27/stable-diffusion</id><content type="html" xml:base="http://localhost:4000/kavysabu1996/jekyll/update/2023/02/27/stable-diffusion.html"><![CDATA[<p>Stable diffusion is an open-source implementation of Latent Diffusion</p>

<p><strong>What exactly is the concept of diffusion in deep learning?</strong> <br />
The process of progressively adding noise to an image is commonly referred to as ‘diffusion’ in the context of deep learning. A deep learning model that utilizes the diffusion process to model the probability distribution of a dataset is commonly referred to as a diffusion model. The diffusion process involves adding progressively more noise to the data to smooth it out, and the model is trained to invert this process by generating a sample from the probability distribution represented by the diffusion process.</p>

<p>The two main processes involved are:</p>

<ol>
  <li>forward diffusion : add noise to the image</li>
  <li>reverse diffusion : remove noise from the image</li>
</ol>

<p>Diffusion models require significant computational resources. To deal with this issue “<a href="https://arxiv.org/pdf/2112.10752.pdf">High-Resolution Image Synthesis with Latent Diffusion Models</a>” introduced latent diffusion models (LDMs). Here, we utilize encoders to reduce the data into a low-dimensional latent space, and then perform the diffusion process on the reduced data. As the resule, both forward and backward diffusions take place in the latent space. Consequently, this approach can significantly reduce the computational cost.</p>

<p>We can rewrite the primary process in Latent Diffusion Models as follows:</p>
<ol>
  <li>forward diffusion : add noise to data in latent space</li>
  <li>reverse diffusion : remove noise from data in latent space</li>
</ol>

<p>references :<br />
<a href="https://www.youtube.com/watch?v=J87hffSMB60">coffee-bean</a> <br />
<a href="https://medium.com/@steinsfu/stable-diffusion-clearly-explained-ed008044e07e">https://medium.com/@steinsfu/stable-diffusion-clearly-explained-ed008044e07e</a> <br />
<a href="https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166">https://medium.com/@steinsfu/diffusion-model-clearly-explained-cd331bd41166</a>.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Stable diffusion is an open-source implementation of Latent Diffusion]]></summary></entry></feed>